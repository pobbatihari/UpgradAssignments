{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import nltk, re, pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint, time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')], [('Rudolph', 'NOUN'), ('Agnew', 'NOUN'), (',', '.'), ('55', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), ('and', 'CONJ'), ('former', 'ADJ'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Consolidated', 'NOUN'), ('Gold', 'NOUN'), ('Fields', 'NOUN'), ('PLC', 'NOUN'), (',', '.'), ('was', 'VERB'), ('named', 'VERB'), ('*-1', 'X'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('British', 'ADJ'), ('industrial', 'ADJ'), ('conglomerate', 'NOUN'), ('.', '.')], [('A', 'DET'), ('form', 'NOUN'), ('of', 'ADP'), ('asbestos', 'NOUN'), ('once', 'ADV'), ('used', 'VERB'), ('*', 'X'), ('*', 'X'), ('to', 'PRT'), ('make', 'VERB'), ('Kent', 'NOUN'), ('cigarette', 'NOUN'), ('filters', 'NOUN'), ('has', 'VERB'), ('caused', 'VERB'), ('a', 'DET'), ('high', 'ADJ'), ('percentage', 'NOUN'), ('of', 'ADP'), ('cancer', 'NOUN'), ('deaths', 'NOUN'), ('among', 'ADP'), ('a', 'DET'), ('group', 'NOUN'), ('of', 'ADP'), ('workers', 'NOUN'), ('exposed', 'VERB'), ('*', 'X'), ('to', 'PRT'), ('it', 'PRON'), ('more', 'ADV'), ('than', 'ADP'), ('30', 'NUM'), ('years', 'NOUN'), ('ago', 'ADP'), (',', '.'), ('researchers', 'NOUN'), ('reported', 'VERB'), ('0', 'X'), ('*T*-1', 'X'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# first four tagged sentences\n",
    "print(nltk_data[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1957\n",
      "1957\n",
      "[[('It', 'PRON'), ('*EXP*-2', 'X'), ('was', 'VERB'), ('just', 'ADV'), ('a', 'DET'), ('stupid', 'ADJ'), ('mistake', 'NOUN'), ('*', 'X'), ('to', 'PRT'), ('get', 'VERB'), ('the', 'DET'), ('license', 'NOUN'), (',', '.'), (\"''\", '.'), ('he', 'PRON'), ('said', 'VERB'), ('0', 'X'), ('*T*-3', 'X'), (',', '.'), ('*-1', 'X'), ('adding', 'VERB'), (',', '.'), ('``', '.'), ('I', 'PRON'), (\"'d\", 'VERB'), ('just', 'ADV'), ('as', 'ADV'), ('soon', 'ADV'), ('not', 'ADV'), ('get', 'VERB'), ('into', 'ADP'), (\"''\", '.'), ('details', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('settlement', 'NOUN'), ('.', '.')], [('Separately', 'ADV'), (',', '.'), ('Reuter', 'NOUN'), ('reported', 'VERB'), ('that', 'ADP'), ('the', 'DET'), ('Papua-New', 'NOUN'), ('Guinea', 'NOUN'), ('government', 'NOUN'), ('urged', 'VERB'), ('its', 'PRON'), ('Parliament', 'NOUN'), ('*-1', 'X'), ('to', 'PRT'), ('extend', 'VERB'), ('a', 'DET'), ('state', 'NOUN'), ('of', 'ADP'), ('emergency', 'NOUN'), ('in', 'ADP'), ('copper-rich', 'ADJ'), ('Bougainville', 'NOUN'), ('Island', 'NOUN'), ('for', 'ADP'), ('two', 'NUM'), ('months', 'NOUN'), ('.', '.')], [('In', 'ADP'), ('*', 'X'), ('investing', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('basis', 'NOUN'), ('of', 'ADP'), ('future', 'ADJ'), ('transactions', 'NOUN'), (',', '.'), ('a', 'DET'), ('role', 'NOUN'), ('often', 'ADV'), ('performed', 'VERB'), ('*', 'X'), ('by', 'ADP'), ('merchant', 'NOUN'), ('banks', 'NOUN'), (',', '.'), ('trading', 'NOUN'), ('companies', 'NOUN'), ('can', 'VERB'), ('cut', 'VERB'), ('through', 'ADP'), ('the', 'DET'), ('logjam', 'NOUN'), ('that', 'ADP'), ('small-company', 'NOUN'), ('owners', 'NOUN'), ('often', 'ADV'), ('face', 'VERB'), ('*T*-1', 'X'), ('with', 'ADP'), ('their', 'PRON'), ('local', 'ADJ'), ('commercial', 'ADJ'), ('banks', 'NOUN'), ('.', '.')], [('In', 'ADP'), ('national', 'ADJ'), ('over-the-counter', 'ADJ'), ('trading', 'NOUN'), ('yesterday', 'NOUN'), (',', '.'), ('POP', 'NOUN'), ('plunged', 'VERB'), ('$', '.'), ('4', 'NUM'), ('*U*', 'X'), ('to', 'PRT'), ('$', '.'), ('14.75', 'NUM'), ('*U*', 'X'), ('.', '.')], [('While', 'ADP'), ('a', 'DET'), ('quarter', 'NOUN'), ('of', 'ADP'), ('black', 'ADJ'), ('voters', 'NOUN'), ('disapprove', 'VERB'), ('of', 'ADP'), ('Mr.', 'NOUN'), ('Bush', 'NOUN'), (\"'s\", 'PRT'), ('handling', 'NOUN'), ('of', 'ADP'), ('his', 'PRON'), ('job', 'NOUN'), (',', '.'), ('only', 'ADV'), ('15', 'NUM'), ('%', 'NOUN'), ('have', 'VERB'), ('a', 'DET'), ('negative', 'ADJ'), ('view', 'NOUN'), ('of', 'ADP'), ('his', 'PRON'), ('spouse', 'NOUN'), ('.', '.'), (\"''\", '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Splitting into train and test\n",
    "random.seed(1234)\n",
    "train_set, test_set = train_test_split(nltk_data,test_size=0.5)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "print(train_set[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50467"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of tagged words\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', '*EXP*-2', 'was', 'just', 'a', 'stupid', 'mistake', '*', 'to', 'get']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens \n",
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8540\n"
     ]
    }
   ],
   "source": [
    "# vocabulary\n",
    "V = set(tokens)\n",
    "print(len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VERB', 'PRON', 'CONJ', 'ADP', 'ADV', 'NOUN', 'NUM', '.', 'X', 'PRT', 'ADJ', 'DET'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tags\n",
    "T = set([pair[1] for pair in train_tagged_words])\n",
    "print(T)\n",
    "len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing P(w/t) and storing in T x V matrix\n",
    "t = len(T)\n",
    "v = len(V)\n",
    "w_given_t = np.zeros((t, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging Algorithm - HMM\n",
    "We'll use the HMM algorithm to tag the words. Given a sequence of words to be tagged, the task is to assign the most probable tag to the word.\n",
    "\n",
    "In other words, to every word w, assign the tag t that maximises the likelihood P(t/w). Since P(t/w) = P(w/t). P(t) / P(w), after ignoring P(w), we have to compute P(w/t) and P(t).\n",
    "\n",
    "P(w/t) is basically the probability that given a tag (say NN), what is the probability of it being w (say 'building'). This can be computed by computing the fraction of all NNs which are equal to w, i.e.\n",
    "\n",
    "P(w/t) = count(w, t) / count(t).\n",
    "\n",
    "The term P(t) is the probability of tag t, and in a tagging task, we assume that a tag will depend only on the previous tag. In other words, the probability of a tag being NN will depend only on the previous tag t(n-1). So for e.g. if t(n-1) is a JJ, then t(n) is likely to be an NN since adjectives often precede a noun (blue coat, tall building etc.).\n",
    "\n",
    "Given the penn treebank tagged dataset, we can compute the two terms P(w/t) and P(t) and store them in two large matrices. The matrix of P(w/t) will be sparse, since each word will not be seen with most tags ever, and those terms will thus be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    \n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
    "for i, t1 in enumerate(list(T)):\n",
    "    for j, t2 in enumerate(list(T)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VERB</th>\n",
       "      <th>PRON</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>.</th>\n",
       "      <th>X</th>\n",
       "      <th>PRT</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>DET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.168923</td>\n",
       "      <td>0.033402</td>\n",
       "      <td>0.005739</td>\n",
       "      <td>0.092996</td>\n",
       "      <td>0.082549</td>\n",
       "      <td>0.109182</td>\n",
       "      <td>0.021189</td>\n",
       "      <td>0.036345</td>\n",
       "      <td>0.219688</td>\n",
       "      <td>0.028546</td>\n",
       "      <td>0.066804</td>\n",
       "      <td>0.134638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.495620</td>\n",
       "      <td>0.004380</td>\n",
       "      <td>0.006569</td>\n",
       "      <td>0.021168</td>\n",
       "      <td>0.034307</td>\n",
       "      <td>0.210949</td>\n",
       "      <td>0.006569</td>\n",
       "      <td>0.032117</td>\n",
       "      <td>0.095620</td>\n",
       "      <td>0.010949</td>\n",
       "      <td>0.073723</td>\n",
       "      <td>0.008029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.143993</td>\n",
       "      <td>0.057420</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.059187</td>\n",
       "      <td>0.052120</td>\n",
       "      <td>0.349823</td>\n",
       "      <td>0.044170</td>\n",
       "      <td>0.026502</td>\n",
       "      <td>0.008834</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.115724</td>\n",
       "      <td>0.136042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.008286</td>\n",
       "      <td>0.068310</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.017987</td>\n",
       "      <td>0.015360</td>\n",
       "      <td>0.324576</td>\n",
       "      <td>0.062854</td>\n",
       "      <td>0.037389</td>\n",
       "      <td>0.033953</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.107922</td>\n",
       "      <td>0.321544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.354997</td>\n",
       "      <td>0.017167</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.123237</td>\n",
       "      <td>0.082771</td>\n",
       "      <td>0.030043</td>\n",
       "      <td>0.029430</td>\n",
       "      <td>0.136726</td>\n",
       "      <td>0.016554</td>\n",
       "      <td>0.015328</td>\n",
       "      <td>0.122624</td>\n",
       "      <td>0.063765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.145238</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>0.041842</td>\n",
       "      <td>0.174908</td>\n",
       "      <td>0.017982</td>\n",
       "      <td>0.262881</td>\n",
       "      <td>0.010512</td>\n",
       "      <td>0.241441</td>\n",
       "      <td>0.029255</td>\n",
       "      <td>0.043502</td>\n",
       "      <td>0.012864</td>\n",
       "      <td>0.013971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.018857</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.013311</td>\n",
       "      <td>0.033278</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.361620</td>\n",
       "      <td>0.184138</td>\n",
       "      <td>0.113145</td>\n",
       "      <td>0.207987</td>\n",
       "      <td>0.024404</td>\n",
       "      <td>0.032169</td>\n",
       "      <td>0.004992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.086034</td>\n",
       "      <td>0.067368</td>\n",
       "      <td>0.058035</td>\n",
       "      <td>0.091804</td>\n",
       "      <td>0.052096</td>\n",
       "      <td>0.226201</td>\n",
       "      <td>0.083659</td>\n",
       "      <td>0.092822</td>\n",
       "      <td>0.028339</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.044799</td>\n",
       "      <td>0.166638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.211620</td>\n",
       "      <td>0.054485</td>\n",
       "      <td>0.009633</td>\n",
       "      <td>0.145996</td>\n",
       "      <td>0.026490</td>\n",
       "      <td>0.060205</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.165864</td>\n",
       "      <td>0.073149</td>\n",
       "      <td>0.179410</td>\n",
       "      <td>0.019868</td>\n",
       "      <td>0.049368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.402431</td>\n",
       "      <td>0.017274</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.020473</td>\n",
       "      <td>0.008317</td>\n",
       "      <td>0.247601</td>\n",
       "      <td>0.057582</td>\n",
       "      <td>0.040947</td>\n",
       "      <td>0.008957</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.094690</td>\n",
       "      <td>0.095969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.012721</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.017685</td>\n",
       "      <td>0.076326</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.700279</td>\n",
       "      <td>0.021098</td>\n",
       "      <td>0.067018</td>\n",
       "      <td>0.017996</td>\n",
       "      <td>0.009618</td>\n",
       "      <td>0.065467</td>\n",
       "      <td>0.005585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.039750</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.008551</td>\n",
       "      <td>0.013404</td>\n",
       "      <td>0.634851</td>\n",
       "      <td>0.021493</td>\n",
       "      <td>0.021031</td>\n",
       "      <td>0.049226</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.201063</td>\n",
       "      <td>0.006240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          VERB      PRON      CONJ       ADP       ADV      NOUN       NUM  \\\n",
       "VERB  0.168923  0.033402  0.005739  0.092996  0.082549  0.109182  0.021189   \n",
       "PRON  0.495620  0.004380  0.006569  0.021168  0.034307  0.210949  0.006569   \n",
       "CONJ  0.143993  0.057420  0.000883  0.059187  0.052120  0.349823  0.044170   \n",
       "ADP   0.008286  0.068310  0.000808  0.017987  0.015360  0.324576  0.062854   \n",
       "ADV   0.354997  0.017167  0.007357  0.123237  0.082771  0.030043  0.029430   \n",
       "NOUN  0.145238  0.005602  0.041842  0.174908  0.017982  0.262881  0.010512   \n",
       "NUM   0.018857  0.001664  0.013311  0.033278  0.004437  0.361620  0.184138   \n",
       ".     0.086034  0.067368  0.058035  0.091804  0.052096  0.226201  0.083659   \n",
       "X     0.211620  0.054485  0.009633  0.145996  0.026490  0.060205  0.003913   \n",
       "PRT   0.402431  0.017274  0.002559  0.020473  0.008317  0.247601  0.057582   \n",
       "ADJ   0.012721  0.000310  0.017685  0.076326  0.005895  0.700279  0.021098   \n",
       "DET   0.039750  0.003467  0.000693  0.008551  0.013404  0.634851  0.021493   \n",
       "\n",
       "             .         X       PRT       ADJ       DET  \n",
       "VERB  0.036345  0.219688  0.028546  0.066804  0.134638  \n",
       "PRON  0.032117  0.095620  0.010949  0.073723  0.008029  \n",
       "CONJ  0.026502  0.008834  0.005300  0.115724  0.136042  \n",
       "ADP   0.037389  0.033953  0.001011  0.107922  0.321544  \n",
       "ADV   0.136726  0.016554  0.015328  0.122624  0.063765  \n",
       "NOUN  0.241441  0.029255  0.043502  0.012864  0.013971  \n",
       "NUM   0.113145  0.207987  0.024404  0.032169  0.004992  \n",
       ".     0.092822  0.028339  0.002036  0.044799  0.166638  \n",
       "X     0.165864  0.073149  0.179410  0.019868  0.049368  \n",
       "PRT   0.040947  0.008957  0.003199  0.094690  0.095969  \n",
       "ADJ   0.067018  0.017996  0.009618  0.065467  0.005585  \n",
       "DET   0.021031  0.049226  0.000231  0.201063  0.006240  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))\n",
    "tags_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running on entire test dataset would take more than 3-4hrs. \n",
    "# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# choose random 5 sents\n",
    "rndom = [random.randint(1,len(test_set)) for x in range(5)]\n",
    "\n",
    "# list of sents\n",
    "test_run = [test_set[i] for i in rndom]\n",
    "\n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  11.360621929168701\n",
      "Vanilla Viterbi Algorithm Accuracy: 0.9115646258503401\n"
     ]
    }
   ],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print(\"Vanilla Viterbi Algorithm Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('deterioration', 'VERB'), ('deterioration', 'NOUN')),\n",
       " (('persons', 'VERB'), ('persons', 'NOUN')),\n",
       " (('*-35', 'VERB'), ('*-35', 'X')),\n",
       " (('notwithstanding', 'VERB'), ('notwithstanding', 'ADP')),\n",
       " (('statutory', 'VERB'), ('statutory', 'ADJ')),\n",
       " (('designations', 'VERB'), ('designations', 'NOUN')),\n",
       " (('credentials', 'VERB'), ('credentials', 'NOUN')),\n",
       " (('benefit', 'NOUN'), ('benefit', 'VERB')),\n",
       " (('roll', 'VERB'), ('roll', 'NOUN')),\n",
       " (('Chinese', 'ADJ'), ('Chinese', 'NOUN')),\n",
       " (('undiplomatic', 'VERB'), ('undiplomatic', 'ADJ')),\n",
       " (('fashion', 'VERB'), ('fashion', 'NOUN')),\n",
       " (('boosters', 'VERB'), ('boosters', 'NOUN'))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the incorrectly tagged words\n",
    "[j for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem of unknown words\n",
    "\n",
    "We can see that all of unknown words have been tagged as 'VERB' as 'VERB' is the first tag in tag list and is assigned if unknown word is encountered (emission probability =0).\n",
    " \n",
    "### First solution for unknown words : \n",
    "### Viterbi Modification-Technique I\n",
    "assign based on transition probabilities only in case of unknown words as emission probability for unknown word is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use transition probability of tags when emission probability is zero (in case of unknown words)\n",
    "\n",
    "def ModifiedViterbi_1(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        p_transition =[] # list for storing transition probabilities\n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            p_transition.append(transition_p)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        state_max = T[p.index(pmax)] \n",
    "        \n",
    "      \n",
    "        # if probability is zero (unknown word) then use transition probability\n",
    "        if(pmax==0):\n",
    "            pmax = max(p_transition)\n",
    "            state_max = T[p_transition.index(pmax)]\n",
    "                        \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  10.19871711730957\n",
      "Modified Viterbi_1 Accuracy:  93.19727891156462\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = ModifiedViterbi_1(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print('Modified Viterbi_1 Accuracy: ',accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('deterioration', 'X'), ('deterioration', 'NOUN')),\n",
       " (('notwithstanding', 'NOUN'), ('notwithstanding', 'ADP')),\n",
       " (('statutory', 'NOUN'), ('statutory', 'ADJ')),\n",
       " (('appearing', 'NOUN'), ('appearing', 'VERB')),\n",
       " (('requesting', 'NOUN'), ('requesting', 'VERB')),\n",
       " (('benefit', 'NOUN'), ('benefit', 'VERB')),\n",
       " (('roll', 'VERB'), ('roll', 'NOUN')),\n",
       " (('Chinese', 'ADJ'), ('Chinese', 'NOUN')),\n",
       " (('undiplomatic', 'VERB'), ('undiplomatic', 'ADJ')),\n",
       " (('fashion', 'X'), ('fashion', 'NOUN'))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the incorrectly tagged words\n",
    "[j for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0] != j[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Modification-Technique II\n",
    "second solution for unknown words:\n",
    "\n",
    "backoff to rule based tagger in case of unknown words.\n",
    "we further observe that POS tag 'X' can be easily encapsulated in regex rule, so we extract it only based on ruled based tagged.\n",
    "Let's define a rule based tagger as below:m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify patterns for tagging\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VERB'),              # gerund\n",
    "    (r'.*ed$', 'VERB'),               # past tense \n",
    "    (r'.*es$', 'VERB'),               # verb    \n",
    "    (r'.*\\'s$', 'NOUN'),              # possessive nouns\n",
    "    (r'.*s$', 'NOUN'),                # plural nouns\n",
    "    (r'\\*T?\\*?-[0-9]+$', 'X'),        # X\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), # cardinal numbers\n",
    "    (r'.*', 'NOUN')                   # nouns\n",
    "]\n",
    "\n",
    "# rule based tagger\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification in Viterbi Algorithm : Backoff to rule based tagger in case unknown word is encountered.\n",
    "def ModifiedViterbi_2(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        state_max = rule_based_tagger.tag([word])[0][1]       \n",
    "       \n",
    "        \n",
    "        if(pmax==0):\n",
    "            state_max = rule_based_tagger.tag([word])[0][1] # assign based on rule based tagger\n",
    "        else:\n",
    "            if state_max != 'X':\n",
    "                # getting state for which probability is maximum\n",
    "                state_max = T[p.index(pmax)]                \n",
    "            \n",
    "        \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  10.337356328964233\n",
      "Modified Viterbi_2 Accuracy:  95.23809523809523\n"
     ]
    }
   ],
   "source": [
    "# tagging the test sentences\n",
    "start = time.time()\n",
    "tagged_seq = ModifiedViterbi_2(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print('Modified Viterbi_2 Accuracy: ',accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('conclude', 'NOUN'), ('conclude', 'VERB')),\n",
       " (('notwithstanding', 'VERB'), ('notwithstanding', 'ADP')),\n",
       " (('statutory', 'NOUN'), ('statutory', 'ADJ')),\n",
       " (('benefit', 'NOUN'), ('benefit', 'VERB')),\n",
       " (('roll', 'VERB'), ('roll', 'NOUN')),\n",
       " (('Chinese', 'ADJ'), ('Chinese', 'NOUN')),\n",
       " (('undiplomatic', 'NOUN'), ('undiplomatic', 'ADJ'))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# let's check the incorrectly tagged words\n",
    "[j for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0] != j[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  10.634554624557495\n",
      "vanilla Viterbi Algorithm Accuracy:  91.15646258503402\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vanilla Viterbi Algorithm\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print('vanilla Viterbi Algorithm Accuracy: ',accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  10.441089868545532\n",
      "Modified Viterbi Algorithm Accuracy:  93.19727891156462\n"
     ]
    }
   ],
   "source": [
    "# Modified Viterbi Algorithm by using transition probability when emission probability is zero\n",
    "start = time.time()\n",
    "tagged_seq = ModifiedViterbi_1(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print('Modified Viterbi Algorithm Accuracy: ',accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  11.14821982383728\n",
      "Modified Viterbi Algorithm Accuracy:  95.23809523809523\n"
     ]
    }
   ],
   "source": [
    "# Modified Viterbi Algorith by using applying rule based \n",
    "start = time.time()\n",
    "tagged_seq = ModifiedViterbi_2(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print('Modified Viterbi Algorithm Accuracy: ',accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following list of words have been correctly POS tagged by ModifiedViterbi_1 as compared to vanilla Viterbi Algorithm:\n",
    "\n",
    "person: correctly tagged as NOUN\n",
    "\n",
    "designations: correctly tagged as NOUN\n",
    "\n",
    "credentials: correctly tagged as NOUN\n",
    "\n",
    "boosters: correctly tagged as NOUN \n",
    "\n",
    "*-35: correctly tagged as 'X'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following list of words have been correctly POS tagged by ModifiedViterbi_2 as compared to  Viterbi_1 and vanilla Viterbi Algorithm:\n",
    " \n",
    "deterioration: correctly tagged as NOUN\n",
    "\n",
    "appearing: correctly tagged as VERB\n",
    "\n",
    "requesting: correctly tagged as VERB\n",
    "\n",
    "fashion: correctly tagged as NOUN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
